# 日報（2025-10-15）

## 作業概要
- LogicBench 環境の英語化・採点パイプラインの整備
- 評価実行とレポート生成（Markdown/TSV）
- GitHub への反映確認と表示崩れ対応
- Gemini 用最小成果物の作成方針整理
- コスト試算・精度集計の下準備

---

## 実施内容（時系列ざっくり）
1. **英語化スクリプト修正**
   - `re.sub()` の置換文字列でのバックスラッシュ崩れ（`bad escape \s`）に遭遇  
     → 置換テーブルを **正規表現の事前コンパイル + 関数置換** に変更して解消。
   - `report_en.md` / `all_results_en.tsv` などを再生成し、日本語混在を除去。

2. **評価実行の動線修正**
   - 期待パスに `run_eval.py` / `merge_reports.py` が見つからずエラー。  
     → 実体は `scripts/run_eval.py` と判明。以降はこのスクリプトを使用。
   - 入力 JSONL に `prompt` が無いサンプルで失敗したため、`jq` で **`prompt` を補完する整形ファイル**（`dev_20_for_run_eval.jsonl`）を作成。

3. **採点のやり直し**
   - 初回は `answer_regex` / `gold` が欠落し **正答率 0%**。  
     → 解析用 Python スクリプトで `gold/regex` を補完し、**再採点 90% (18/20)** に改善。  
     - 生成物：  
       - `all_results_scored.tsv` / `all_results_scored_flat.tsv` / `all_results_scored_gold.tsv`  
       - フラットTSVに対し **再採点版** `all_results_scored_flat_scored.tsv` を作成。

4. **レポート生成**
   - RUN `20251015_150556` の最終レポートを生成：  
     - **Overall accuracy: 90.00% (18/20)**  
     - カテゴリ：`math` のみ  
     - Misses: `rebuild:016`, `rebuild:019`
   - GitHub 反映で **レンダリング差**（パイプ `|` を含む正規表現表示）に起因する見え方の不一致があり、  
     - **パイプをエスケープ** して Markdown テーブルが崩れないよう調整  
     - `INDEX.md` へエントリ追加（恒久的に辿れる導線を確保）

5. **Gemini 取り込み準備**
   - ZIP を最小構成で渡す提案（`data/` 入力、`scripts/run_eval.py`、`runs/<RUN>/必要TSV`、`evalbench/reports/<RUN>/…` など）。
   - 予測結果 TSV の**列名仕様**を整理：  
     - 正：`id	final_answer	input_tokens	output_tokens`  
     - ヘッダに角カッコは付けない（シェル衝突回避）。

---

## 詰まった点と対応
- **正規表現置換のバックスラッシュ解釈**  
  → コンパイル済みパターン + ラムダ置換で「置換文字列をリテラル扱い」へ。
- **スクリプトのパス相違**（`logicbench-eval/`直下想定 vs `scripts/`実体）  
  → 呼び出し先を `scripts/run_eval.py` に統一。
- **`prompt` 欠落で評価不能**  
  → `jq` で `question/prompt` の候補から補完する整形パイプ追加。
- **採点 0% 問題**（`gold/regex` 無し）  
  → gold/regex の補完＆再採点ロジックを追加し、90% に回復。
- **GitHub 上の表示不一致**（テーブル内の `|` で崩れる）  
  → `|` をエスケープして **表レイアウトを固定**、`INDEX.md` からの導線も整備。
- **`jq` エラー（`.[] | …` と `-Rc` の使い分け）**  
  → 行単位 JSONL の場合は **`-Rc` で `fromjson`** を明示する形に修正。

---

## 成果物
- 評価結果：**RUN = 20251015_150556**（Overall 90.00%）
- レポート一式（`evalbench/reports/20251015_150556/`）
  - `report.md`（表崩れ対策済み）
  - `all_results.tsv` / `per_category.tsv` / `misses.tsv`
- 採点用 TSV 群（`logicbench-eval/runs/20251015_150556/`）
  - `all_results_scored.tsv` / `all_results_scored_flat.tsv` / `all_results_scored_gold.tsv`  
  - `all_results_scored_flat_scored.tsv`（再採点済み）
- 整形済み入力：`data/dev_20_for_run_eval_math.jsonl`
- Gemini 予測用テンプレ：`results/gemini_predictions.tsv` 仕様定義

---

## 既知のリスク / 追加でやると良いこと
- **レンダリング由来の表示揺れ**は引き続き発生し得るため、  
  - レポート生成時に **正規表現中の `|` を常にエスケープ**する処理をビルトイン化。
- **評価スクリプトの I/O 仕様**を README に明文化（`prompt` 必須／欠落時の整形フィルタ等）。
- **Gemini 結果の取り込みスクリプト**を作成（`results/gemini_predictions.tsv` を読み、既存スコアリングと統合する CLI を用意）。

---

## 明日のアクション（提案）
1. Gemini の実予測 TSV を取り込み、**同一 RUN で OpenAI と比較レポート**を自動生成。
2. レポートの **コスト試算セクション**を数式化（モデル別レート差し替え可能なテンプレ）。
3. `scripts/run_eval.py` に **エラーメッセージ改善**（`--input` の渡し方、必須列の説明）。

---

## 所感・学び
- Markdown テーブル内の正規表現は **人間にもレンダラにも厳しい**。可読性と堅牢性のバランス取りが重要。  
- JSONL の **1行＝1 JSON** という前提が壊れると整形コストが増えるため、最初に **`jq -Rc fromjson`** を徹底するのが安全。  
- 採点は **`gold` と `regex` の管理がすべて**。不足時フォールバック（厳密一致）を用意するとデバッグが速い。  
